# -*- coding: utf-8 -*-
"""CODSOFT - 1 (Titanic Survival Prediction Project)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dfFGCCuEGYNfQtOLFxHAlwyPP_81cF9I

# **Titanic Survival Prediction**

### **This project involves a complete data science workflow to predict whether a passenger on the Titanic survived.**

## **Importing Libraries**
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix, classification_report
import matplotlib.pyplot as plt
import seaborn as sns

"""## **Data Loading and Exploration**

"""

# Load the dataset
df = pd.read_csv('Titanic-Dataset.csv')

# Display the first 5 rows and a summary of the data
print("First 5 rows of the dataset:")
print(df.head().to_markdown(index=False, numalign="left", stralign="left"))
print("\n")
print("Data information:")
print(df.info())

"""The first step was to load the Titanic-Dataset.csv file and perform an initial analysis. The data contains 891 entries and 12 columns, and we identified missing values in the Age, Cabin, and Embarked columns.

## **Data Preprocessing**
"""

# Data Preprocessing

# 1. Handling Missing Values
# Drop the 'Cabin' column due to a large number of missing values
df.drop('Cabin', axis=1, inplace=True)

# Fill missing 'Age' values with the median age
df['Age'].fillna(df['Age'].median(), inplace=True)

# Fill missing 'Embarked' values with the mode
df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)

# 2. Feature Engineering
# Create a 'FamilySize' feature
df['FamilySize'] = df['SibSp'] + df['Parch'] + 1

# Create a 'IsAlone' feature
df['IsAlone'] = (df['FamilySize'] == 1).astype(int)

# Drop irrelevant columns
df.drop(['PassengerId', 'Name', 'Ticket', 'SibSp', 'Parch'], axis=1, inplace=True)

# 3. Encoding Categorical Features
# Convert 'Sex' to numerical format (0 for female, 1 for male)
df['Sex'] = df['Sex'].map({'female': 0, 'male': 1})

# Use one-hot encoding for 'Embarked'
df = pd.get_dummies(df, columns=['Embarked'], drop_first=True)

# Display the cleaned and processed DataFrame
print("Cleaned and processed DataFrame information:")
print(df.info())
print("\nFirst 5 rows of the cleaned dataset:")
print(df.head().to_markdown(index=False, numalign="left", stralign="left"))

# Prepare data for modeling
X = df.drop('Survived', axis=1)
y = df['Survived']

# Save the preprocessed data to a CSV file for user to download
df.to_csv("titanic_preprocessed.csv", index=False)

"""**To prepare the data for modeling, the following steps were taken:**

1. The Cabin column was dropped due to a high number of missing values.

2. The missing values in the Age column were filled with the median age.

3. The two missing values in the Embarked column were filled with the most frequent value (mode).

4. New features were engineered: FamilySize (by combining SibSp and Parch) and IsAlone.

5. Irrelevant columns like PassengerId, Name, Ticket, SibSp, and Parch were removed.

6. Categorical features (Sex and Embarked) were converted into a numerical format. Sex was mapped to 0 and 1, and Embarked was one-hot encoded.

## **Model Building and Evaluation**
"""

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train the RandomForestClassifier model
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the model
print("Classification Report:")
print(classification_report(y_test, y_pred))

"""**The preprocessed data was split into training and testing sets (80% and 20% respectively).**

 A **RandomForestClassifier** was chosen as the model for this task.

After training the model and making predictions on the test set, the following performance metrics were obtained:

* Accuracy: 0.83

* Precision: 0.86 for not survived (class 0) and 0.80 for survived (class 1).

* Recall: 0.86 for not survived (class 0) and 0.80 for survived (class 1).

* F1-Score: 0.86 for not survived (class 0) and 0.80 for survived (class 1).
"""

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(conf_matrix)

# Plot the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Survived', 'Survived'], yticklabels=['Not Survived', 'Survived'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.savefig('confusion_matrix.png')
plt.show()

"""**The confusion matrix below provides a visual summary of the model's predictions:**

* **True Negatives** (Not Survived, predicted Not Survived): 90

* **False Positives** (Not Survived, predicted Survived): 15

* **False Negatives** (Survived, predicted Not Survived): 15

* **True Positives** (Survived, predicted Survived): 59

This project demonstrates a complete workflow from data cleaning to model evaluation, providing a solid foundation for more advanced analysis.
"""